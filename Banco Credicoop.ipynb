{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraccion de datos ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXTRACCION DE DATOS\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Convertir PDF a imágenes\n",
    "pages = convert_from_path('Extracto Banco Credicoop.pdf', 300)  # Utilizar el PDF con líneas\n",
    "# Definir el área de interés (x1, y1, x2, y y2)\n",
    "area_of_interest = {\"x1\": 25, \"y1\": 900, \"x2\": 3100, \"y2\": 3800}\n",
    "\n",
    "# Configuración personalizada de pytesseract\n",
    "custom_config = r'--oem 3 --psm 6 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz.-,:;/ -c preserve_interword_spaces=1 -c user_defined_dpi=300'\n",
    "\n",
    "# Lista para almacenar los DataFrames resultantes\n",
    "lista_de_dataframes = []\n",
    "\n",
    "# Recorrer cada página convertida en imagen\n",
    "for page_number, page in enumerate(pages):\n",
    "    # Convertir la imagen de PIL a formato OpenCV\n",
    "    page_cv = np.array(page)\n",
    "    page_cv = cv2.cvtColor(page_cv, cv2.COLOR_RGB2GRAY)  # Convertir a escala de grises\n",
    "\n",
    "    # Extraer las coordenadas del área de interés y recortar\n",
    "    x1, y1, x2, y2 = area_of_interest[\"x1\"], area_of_interest[\"y1\"], area_of_interest[\"x2\"], area_of_interest[\"y2\"]\n",
    "    cropped = page_cv[y1:y2, x1:x2]\n",
    "\n",
    "    # Preprocesar la imagen\n",
    "    _, binary = cv2.threshold(cropped, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)  # Binarización\n",
    "    denoised = cv2.fastNlMeansDenoising(binary, None, 30, 7, 21)  # Eliminación de ruido\n",
    "    contrast = cv2.convertScaleAbs(denoised, alpha=1.5, beta=0)  # Aumento de contraste\n",
    "\n",
    "    # Extraer texto del área preprocesada\n",
    "    data = pytesseract.image_to_data(contrast, output_type=pytesseract.Output.DICT, lang=\"spa\", config=custom_config)\n",
    "\n",
    "    # Convertir el diccionario resultante en un DataFrame y agregarlo a la lista\n",
    "    df_temp = pd.DataFrame(data)\n",
    "    df_temp['Page'] = page_number + 1  # Añadir columna de página\n",
    "    lista_de_dataframes.append(df_temp)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_pytessect = pd.concat(lista_de_dataframes, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de datos ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtrar_por_pagina(df, pagina):\n",
    "    return df[df['Page'] == pagina]  # Filtrar el DataFrame por el número de página.\n",
    "\n",
    "def cortar_desde_hasta(df, start_text, end_text):\n",
    "    # Verificar si existen las palabras clave en el DataFrame\n",
    "    if not df['text'].str.contains(start_text).any():\n",
    "        return pd.DataFrame()  # Retornar un DataFrame vacío si no existe 'start_text'\n",
    "    start_index = df[df['text'].str.contains(start_text)].index[0]  # Índice de la primera aparición del texto de inicio\n",
    "    if not df['text'].str.contains(end_text).any():\n",
    "        return pd.DataFrame()  # Retornar un DataFrame vacío si no existen 'end_text'\n",
    "    end_index = df[(df['text'].str.contains(end_text)) & (df.index > start_index)].index[0]  # Índice de la primera aparición del texto de fin después del inicio\n",
    "    return df.loc[start_index:end_index]  # Cortar el DataFrame entre los índices encontrados\n",
    "\n",
    "# Procesar cada página\n",
    "max_page = df_pytessect['Page'].max()\n",
    "lista_de_paginas = []\n",
    "for page in range(1, max_page + 1):\n",
    "    df_pagina = filtrar_por_pagina(df_pytessect, page)\n",
    "    df_cortado = cortar_desde_hasta(df_pagina, \"FECHA\", \"CONTINUA\")\n",
    "    if not df_cortado.empty:\n",
    "        lista_de_paginas.append(df_cortado)\n",
    "\n",
    "# Concatenar todos los DataFrames cortados en uno solo\n",
    "df_filtrado = pd.concat(lista_de_paginas, ignore_index=True)\n",
    "\n",
    "# Filtrar la última página\n",
    "max_page2 = df_filtrado['Page'].max()-1\n",
    "df_Resto = df_filtrado[df_filtrado['Page'] < max_page2]\n",
    "df_Last = df_filtrado[df_filtrado['Page'] == max_page2]\n",
    "df_Last_filter = cortar_desde_hasta(df_Last, \"FECHA\", \"DENOMINACION\")\n",
    "df_limpio = pd.concat([df_Resto, df_Last_filter], ignore_index=True)\n",
    "df_table = df_limpio[df_limpio['text'] != \"\"][:-1]\n",
    "df_table = df_table[~df_table['text'].str.contains(\"CONTINUA\")]\n",
    "\n",
    "# Calcular las coordenadas absolutas\n",
    "df_table['Esq Sup Izq x1'] = df_table['left'] + area_of_interest['x1']\n",
    "\n",
    "# Clasificar las columnas por coordenadas\n",
    "def calcular_valor(x):\n",
    "    if x > 2650:\n",
    "        return 6\n",
    "    elif x > 2150:\n",
    "        return 5\n",
    "    elif x > 1700:\n",
    "        return 4\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_table['Column'] = df_table['Esq Sup Izq x1'].apply(calcular_valor)\n",
    "df_concatenado = df_table.groupby(['Page', 'par_num', 'line_num', 'Column'], as_index=False).agg({'text': ' '.join})\n",
    "df_pivoted = df_concatenado.pivot(index=['Page', 'par_num', 'line_num'], columns='Column', values='text')\n",
    "df_pivoted.reset_index(inplace=True)\n",
    "\n",
    "# Dividir la columna 'text' en 'Fecha' y 'Resto del Texto'\n",
    "def split_text(row):\n",
    "    if re.match(r'\\d{2}/\\d{2}/\\d{2}', row[0][:8]):\n",
    "        return pd.Series([row[0][:8], row[0][8:].strip()])\n",
    "    else:\n",
    "        return pd.Series(['', row[0]])\n",
    "\n",
    "df_pivoted[['Fecha', 'Resto del Texto']] = df_pivoted.apply(split_text, axis=1)\n",
    "\n",
    "# Separar números y texto en 'Resto del Texto'\n",
    "def separar_numeros_texto(row):\n",
    "    if row['Fecha']:\n",
    "        match = re.match(r\"(\\d+)(.*)\", row['Resto del Texto'])\n",
    "        if match:\n",
    "            return pd.Series([match.group(1), match.group(2).strip()])\n",
    "        else:\n",
    "            return pd.Series(['', row['Resto del Texto']])\n",
    "    else:\n",
    "        return pd.Series(['', row['Resto del Texto']])\n",
    "\n",
    "df_pivoted[['Comprobante', 'Descripcion']] = df_pivoted.apply(separar_numeros_texto, axis=1)\n",
    "df_banco = df_pivoted.drop(columns=['par_num', 'line_num', 0, \"Resto del Texto\"])\n",
    "\n",
    "# Insertar espacios en la columna 'Descripcion'\n",
    "def insertar_espacios(texto):\n",
    "    texto = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', texto)\n",
    "    texto = re.sub(r'(?<=[a-zA-Z])(?=\\d)', ' ', texto)\n",
    "    texto = re.sub(r'(?<=\\d)(?=[a-zA-Z])', ' ', texto)\n",
    "    texto = re.sub(r'(\\d{2}/\\d{2})(\\d{2}:\\d{2})', r'\\1 \\2', texto)\n",
    "    return texto\n",
    "\n",
    "df_banco['Descripcion'] = df_banco['Descripcion'].apply(insertar_espacios)\n",
    "df_banco = df_banco[~df_banco['Descripcion'].str.contains(\"FECHA\")]\n",
    "df_banco.rename(columns={4: 'Debitos', 5: 'Creditos', 6: 'Saldo'}, inplace=True)\n",
    "df_banco = df_banco[['Page', 'Fecha', 'Comprobante', 'Descripcion', 'Debitos', 'Creditos', 'Saldo']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraccion de Informacion Adicional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame guardado en 'Banco 30-57142135-2_CBU 1910142455014251145146_2019-03-31 a 2018-10-01.csv'\n"
     ]
    }
   ],
   "source": [
    "# Convertir la primera página del PDF a imagen\n",
    "page0 = pages[0]\n",
    "page_cv0 = np.array(page0)\n",
    "page_cv0 = cv2.cvtColor(page_cv0, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Recortar el área de interés y preprocesar la imagen\n",
    "x0_1, y0_1, x0_2, y0_2 = 35, 600, 3000, 1400\n",
    "cropped0 = page_cv0[y0_1:y0_2, x0_1:x0_2]\n",
    "_, binary0 = cv2.threshold(cropped0, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "denoised0 = cv2.fastNlMeansDenoising(binary0, None, 30, 7, 21)\n",
    "contrast0 = cv2.convertScaleAbs(denoised0, alpha=1.5, beta=0)\n",
    "pagina0 = pytesseract.image_to_data(contrast0, output_type=pytesseract.Output.DICT, lang=\"spa\", config=custom_config)\n",
    "\n",
    "# Extraer CBU y banco\n",
    "col_text = pd.DataFrame(pagina0['text'])\n",
    "CBU = col_text.iloc[-1][0][-22:]  # Extraer los últimos 22 caracteres como CBU\n",
    "banco = col_text[col_text[0].str.contains('CUIT')].iloc[0]\n",
    "banco = \"Banco \" + banco[0][-13:]  # Formatear el nombre del banco\n",
    "\n",
    "# Extraer y formatear el periodo\n",
    "Periodo_text = col_text[col_text[0].str.contains('Cta')].iloc[0][0]\n",
    "fechas = re.findall(r'\\d{2}/\\d{2}/\\d{4}', Periodo_text)\n",
    "fecha1 = '-'.join(fechas[0].split('/')[::-1])\n",
    "fecha2 = '-'.join(fechas[1].split('/')[::-1])\n",
    "periodo = f\"{fecha2} a {fecha1}\"\n",
    "\n",
    "# Crear nombre del archivo Excel basado en CBU y periodo\n",
    "name_excel = f\"{banco}_CBU {CBU}_{periodo}.csv\"\n",
    "\n",
    "# Guardar el DataFrame en un archivo Excel\n",
    "df_banco.to_csv(name_excel, sep=';', encoding='latin-1', index=False)\n",
    "print(f\"DataFrame guardado en '{name_excel}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Extract_full",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
